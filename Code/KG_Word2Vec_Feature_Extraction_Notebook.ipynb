{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab495af",
   "metadata": {},
   "source": [
    "# Knowledge Graph Word2Vec Pipeline\n",
    "\n",
    "This notebook reproduces the experimental data preparation pipeline described in the paper.  \n",
    "It:\n",
    "\n",
    "1. Loads and cleans the knowledge graph.\n",
    "2. Removes all edges used in train/test sets from the knowledge graph.\n",
    "3. Trains a Word2Vec model on the cleaned graph.\n",
    "4. Builds train and test feature matrices (`X.npy`, `X_test.npy`) and label vectors (`y.npy`, `Y_test.npy`).\n",
    "\n",
    "> **Important:** Run all cells in order from top to bottom. Make sure the relative paths in the code match the structure of your project repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0ab79",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6da182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment and run if needed)\n",
    "# !pip install pandas numpy gensim nltk scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e8852",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "This cell imports all libraries used in the pipeline and downloads the NLTK tokenizer models (once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# Download tokenizer data (run once)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b3b0b",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess the Knowledge Graph\n",
    "\n",
    "This step loads the knowledge graph from CSV, removes duplicated rows and unnecessary columns,  \n",
    "and normalizes entity prefixes for compounds, proteins, and diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Preprocess Knowledge Graph\n",
    "knowledge_graph_path = \"./data/Knowledge_graph.csv\"\n",
    "df = pd.read_csv(knowledge_graph_path)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Replace unwanted prefixes\n",
    "prefix_replacements = {\n",
    "    'Compound::': 'Compound_',\n",
    "    'protein::': 'protein_',\n",
    "    'Disease::': 'Disease_'\n",
    "}\n",
    "for key, value in prefix_replacements.items():\n",
    "    df['source'] = df['source'].str.replace(key, value)\n",
    "    df['target'] = df['target'].str.replace(key, value)\n",
    "\n",
    "print(\"Knowledge Graph shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4a0ea",
   "metadata": {},
   "source": [
    "## 4. Load Positive and Negative Train-Test Data\n",
    "\n",
    "In this step, we load the positive and negative **test** interaction pairs.  \n",
    "These will later be removed from the knowledge graph to avoid data leakage and will be used to build test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Positive and Negative Train-Test Data\n",
    "positive_train_path = \"./data/positive_test_df.csv\"\n",
    "negative_train_path = \"./data/negative_test_df.csv\"\n",
    "positive_df = pd.read_csv(positive_train_path)\n",
    "negative_df = pd.read_csv(negative_train_path)\n",
    "\n",
    "# Rename columns for consistency\n",
    "positive_df.rename(columns={\"drug_id\": \"source\", \"ind_id\": \"target\"}, inplace=True)\n",
    "negative_df.rename(columns={\"drug_id\": \"source\", \"ind_id\": \"target\"}, inplace=True)\n",
    "\n",
    "positive_df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "negative_df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Display dataset shapes\n",
    "print(\"Positive Train-Test shape:\", positive_df.shape)\n",
    "print(\"Negative Train-Test shape:\", negative_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d1b4f",
   "metadata": {},
   "source": [
    "## 5. Remove Train/Test Edges from the Knowledge Graph\n",
    "\n",
    "To prevent information leakage, we remove all edges from the knowledge graph that are used in:  \n",
    "- positive interactions (`has_approved_interaction`)  \n",
    "- negative interactions (`has_side_effect`)  \n",
    "\n",
    "We identify such edges with a composite key (`source_target`) and drop them from the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Filter Rows to Delete from Knowledge Graph\n",
    "\n",
    "# Create composite keys for matching\n",
    "df['composite_key'] = df['source'] + '_' + df['target']\n",
    "positive_df['composite_key'] = positive_df['source'] + '_' + positive_df['target']\n",
    "negative_df['composite_key'] = negative_df['source'] + '_' + negative_df['target']\n",
    "\n",
    "# Identify rows to delete\n",
    "df_to_delete_positive = df[(df['relation'] == 'has_approved_interaction') &\n",
    "                           (df['composite_key'].isin(positive_df['composite_key']))]\n",
    "\n",
    "df_to_delete_negative = df[(df['relation'] == 'has_side_effect') &\n",
    "                           (df['composite_key'].isin(negative_df['composite_key']))]\n",
    "\n",
    "df_to_delete = pd.concat([df_to_delete_positive, df_to_delete_negative])\n",
    "df_cleaned = df.drop(df_to_delete.index)\n",
    "df_cleaned.drop(columns=['composite_key'], inplace=True)\n",
    "\n",
    "# Save cleaned DataFrame\n",
    "df_cleaned_path = \"./output/train_df_Word2Vec.csv\"\n",
    "df_cleaned.to_csv(df_cleaned_path, index=False)\n",
    "print(\"Cleaned Knowledge Graph shape:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a6e90",
   "metadata": {},
   "source": [
    "## 6. Construct Sentences and Train Word2Vec\n",
    "\n",
    "Here we convert each triple in the cleaned knowledge graph into a simple sentence of the form:\n",
    "\n",
    "`source relation target .`\n",
    "\n",
    "We then tokenize these sentences and train a Word2Vec model to obtain dense embeddings for all entities and relations in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ddde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare Sentences for Word2Vec\n",
    "sentences = [f\"{row['source']} {row['relation']} {row['target']} .\" for _, row in df_cleaned.iterrows()]\n",
    "sentences_tokenized = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Step 5: Train Word2Vec Model\n",
    "model = Word2Vec(vector_size=650, window=2, sg=0, min_count=1, epochs=100, alpha=0.001)\n",
    "model.build_vocab(sentences_tokenized, progress_per=1500)\n",
    "model.train(sentences_tokenized, total_examples=model.corpus_count, epochs=500, report_delay=1)\n",
    "model.save(\"./output/model1.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a446e",
   "metadata": {},
   "source": [
    "## 7. Build Training Features (`X.npy`) and Labels (`y.npy`)\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "1. Load **positive** and **negative** training interaction pairs.\n",
    "2. Apply the same prefix cleaning to drug and disease identifiers.\n",
    "3. Concatenate the drug and disease Word2Vec vectors to form a single feature vector per pair.\n",
    "4. Create the label vector `y` (1 for positive, 0 for negative) and save both `X` and `y` as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Prepare X.npy and y.npy for Training\n",
    "positive_train_path = \"./data/positive_train_df.csv\"\n",
    "negative_train_path = \"./data/negative_train_df.csv\"\n",
    "has = pd.read_csv(positive_train_path)\n",
    "hasnt = pd.read_csv(negative_train_path)\n",
    "\n",
    "# Rename columns and clean prefixes\n",
    "has.rename(columns={\"source\": \"drug_id\", \"target\": \"ind_id\"}, inplace=True)\n",
    "hasnt.rename(columns={\"source\": \"drug_id\", \"target\": \"ind_id\"}, inplace=True)\n",
    "for key, value in prefix_replacements.items():\n",
    "    has['drug_id'] = has['drug_id'].str.replace(key, value)\n",
    "    has['ind_id'] = has['ind_id'].str.replace(key, value)\n",
    "    hasnt['drug_id'] = hasnt['drug_id'].str.replace(key, value)\n",
    "    hasnt['ind_id'] = hasnt['ind_id'].str.replace(key, value)\n",
    "\n",
    "# Concatenate positive and negative data\n",
    "frames = [has, hasnt]\n",
    "f = pd.concat(frames)\n",
    "\n",
    "# Generate feature vectors: concat(drug_embedding, disease_embedding)\n",
    "X = [\n",
    "    np.concatenate((model.wv[row['drug_id']].reshape((1, 650)),\n",
    "                    model.wv[row['ind_id']].reshape((1, 650))), axis=None)\n",
    "    for _, row in f.iterrows()\n",
    "]\n",
    "\n",
    "# Create labels: 1 for positive, 0 for negative\n",
    "y = np.zeros(len(X))\n",
    "y[:len(has)] = 1\n",
    "\n",
    "np.save(\"./output/X.npy\", X)\n",
    "np.save(\"./output/y.npy\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967fab7",
   "metadata": {},
   "source": [
    "## 8. Build Test Features (`X_test.npy`) and Labels (`Y_test.npy`)\n",
    "\n",
    "Finally, we repeat the same feature construction procedure for the **test** positive and negative interaction pairs:\n",
    "\n",
    "- Load the positive and negative test sets.\n",
    "- Concatenate the corresponding drug and disease embeddings.\n",
    "- Assign labels (1 for positive, 0 for negative).\n",
    "- Save `X_test` and `Y_test` as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Prepare Test Data\n",
    "positive_test_path = \"./data/positive_test_df.csv\"\n",
    "negative_test_path = \"./data/negative_test_df.csv\"\n",
    "positive_test_df = pd.read_csv(positive_test_path)\n",
    "negative_test_df = pd.read_csv(negative_test_path)\n",
    "\n",
    "# Add positive and negative test samples to test set\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for _, row in positive_test_df.iterrows():\n",
    "    X_test.append(\n",
    "        np.concatenate((model.wv[row['drug_id']].reshape((1, 650)),\n",
    "                        model.wv[row['ind_id']].reshape((1, 650))), axis=None)\n",
    "    )\n",
    "    Y_test.append(1)\n",
    "\n",
    "for _, row in negative_test_df.iterrows():\n",
    "    X_test.append(\n",
    "        np.concatenate((model.wv[row['drug_id']].reshape((1, 650)),\n",
    "                        model.wv[row['ind_id']].reshape((1, 650))), axis=None)\n",
    "    )\n",
    "    Y_test.append(0)\n",
    "\n",
    "np.save(\"./output/X_test.npy\", X_test)\n",
    "np.save(\"./output/Y_test.npy\", Y_test)\n",
    "\n",
    "# Final Message\n",
    "print(\"All tasks completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2f2aa",
   "metadata": {},
   "source": [
    "## 9. Notes\n",
    "\n",
    "- The `./data/` folder should contain all input CSV files:\n",
    "  - `Knowledge_graph.csv`\n",
    "  - `positive_train_df.csv`, `negative_train_df.csv`\n",
    "  - `positive_test_df.csv`, `negative_test_df.csv`\n",
    "- The `./output/` folder will be created automatically (if it does not exist, please create it) and will store:\n",
    "  - `train_df_Word2Vec.csv`\n",
    "  - `model1.model`\n",
    "  - `X.npy`, `y.npy`\n",
    "  - `X_test.npy`, `Y_test.npy`\n",
    "\n",
    "You can now use these NumPy arrays as input to your downstream models (e.g., IDC_Conv1D or other classifiers)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
