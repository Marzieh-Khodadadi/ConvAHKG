{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16da0c48",
   "metadata": {},
   "source": [
    "# IDC_Conv1D Model Training and Evaluation\n",
    "\n",
    "This notebook reproduces the experiments for the IDC_Conv1D model used for drug–disease link prediction. It covers environment configuration, data loading, model definition, training with stratified K-fold cross-validation, and evaluation on a held-out test set, including ROC/PR curves and detailed metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb5efa",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "This cell lists the main Python package dependencies. You can install them into a fresh environment (e.g., via `conda` or `venv`).\n",
    "\n",
    "**Example (pip):**\n",
    "```bash\n",
    "pip install numpy pandas matplotlib scikit-learn tensorflow\n",
    "```\n",
    "\n",
    "Make sure you also have a GPU-enabled TensorFlow installation if you want to reproduce the GPU-based experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9f3f9",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Configure GPU\n",
    "\n",
    "This cell imports all required libraries (TensorFlow/Keras, NumPy, Pandas, scikit-learn, and Matplotlib) and configures the GPU for controlled memory growth, as used in the original experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcbeec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, Conv1D, MaxPooling1D,\n",
    "                                     Flatten, Reshape, UpSampling1D, Concatenate)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import (roc_auc_score, precision_recall_curve, auc, brier_score_loss,\n",
    "                             cohen_kappa_score, matthews_corrcoef, f1_score, recall_score,\n",
    "                             accuracy_score, precision_score, roc_curve)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# (Optional) List local devices including GPUs\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# Basic GPU configuration (if a GPU is available)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except Exception as e:\n",
    "        print('Could not set memory growth:', e)\n",
    "\n",
    "# Additional GPU configuration (TensorFlow v1-style session, optional)\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.05)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257dd098",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "This cell loads the preprocessed training and test datasets from `.npy` files. `X` arrays contain concatenated drug and disease feature vectors, and `y` arrays contain the corresponding labels.\n",
    "\n",
    "- `X_train_0`, `y_train_0`: Training features and labels\n",
    "- `X_test_0`, `Y_test_0`: Held-out test features and labels\n",
    "\n",
    "Update the file paths if your data is stored in a different directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ea161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "X_test_0 = np.load('./data/X_me.npy')\n",
    "Y_test_0 = np.load('./data/Y_me.npy')\n",
    "X_train_0 = np.load('./data/X.npy')\n",
    "y_train_0 = np.load('./data/y.npy')\n",
    "\n",
    "print('X_train shape:', X_train_0.shape)\n",
    "print('y_train shape:', y_train_0.shape)\n",
    "print('X_test shape:', X_test_0.shape)\n",
    "print('Y_test shape:', Y_test_0.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d704922f",
   "metadata": {},
   "source": [
    "## 4. Loss Function and Basic Plotting Utilities\n",
    "\n",
    "This cell defines:\n",
    "- `calculate_class_weights`: Computes positive and negative class weights based on label distribution.\n",
    "- `weighted_binary_crossentropy_loss`: A weighted binary cross-entropy loss that handles class imbalance.\n",
    "- `plot_roc_pr_curves`: A helper function to plot ROC and Precision–Recall curves for a single prediction run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weighted loss and basic ROC/PR plotting\n",
    "def calculate_class_weights(y_true):\n",
    "    pos_weight = tf.reduce_sum(1 - y_true) / tf.reduce_sum(y_true)\n",
    "    neg_weight = 1.0\n",
    "    return pos_weight, neg_weight\n",
    "\n",
    "def weighted_binary_crossentropy_loss(y_true, y_pred, pos_weight=None, neg_weight=None):\n",
    "    if pos_weight is None or neg_weight is None:\n",
    "        pos_weight, neg_weight = calculate_class_weights(y_true)\n",
    "    \n",
    "    y_true = tf.cast(y_true, dtype=tf.float64)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float64)\n",
    "    \n",
    "    # Binary cross-entropy loss\n",
    "    bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Apply weights\n",
    "    weighted_loss = bce_loss * (y_true * pos_weight + (1 - y_true) * neg_weight)\n",
    "    \n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "def plot_roc_pr_curves(y_true, y_pred, file_prefix):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # ROC Curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc_score(y_true, y_pred))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, label='Precision-Recall (AUC = %0.2f)' % auc(recall, precision))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='lower left')\n",
    "    \n",
    "    plt.savefig(f'{file_prefix}_roc_pr_curve.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f1287",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation Plotting Utilities\n",
    "\n",
    "This cell defines helper functions to visualize model performance across K-folds:\n",
    "- `plot_roc_pr_curves_all_folds`: Plots individual and mean ROC/PR curves over all folds.\n",
    "- `model_history`: Plots training and validation accuracy/loss per epoch for each fold.\n",
    "- `plot_fold_pr_curves`: An additional function to visualize individual and mean PR curves across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42805ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score, auc\n",
    "\n",
    "def plot_roc_pr_curves_all_folds(folds_roc, folds_pr, mean_fpr, mean_tpr, mean_recall, mean_precision, file_prefix):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    cmap = plt.get_cmap('tab10')  \n",
    "    # Plot all ROC Curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for fold, (fpr, tpr) in enumerate(folds_roc, 1):\n",
    "        plt.plot(fpr, tpr, lw=1.2, color=cmap(fold % 10), alpha=0.6,\n",
    "                 label=f'Fold {fold} ROC (AUC = {auc(fpr, tpr):.2f})')\n",
    "    plt.plot(mean_fpr, mean_tpr, linestyle='--', lw=1.5,\n",
    "             label=f'Mean ROC (AUC = {auc(mean_fpr, mean_tpr):.2f})', color='black')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # Plot all Precision-Recall Curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for fold, (precision, recall) in enumerate(folds_pr, 1):\n",
    "        plt.plot(recall, precision, lw=1.2, alpha=0.6, color=cmap(fold % 10),\n",
    "                 label=f'Fold {fold} PR (AUC = {auc(recall, precision):.2f})')\n",
    "    plt.plot(mean_recall, mean_precision, linestyle='--', lw=1.5,\n",
    "             label=f'Mean PR (AUC = {auc(mean_recall, mean_precision):.2f})', color='black')\n",
    "    plt.xlim([-0.05, 1.0])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='lower left')\n",
    "\n",
    "    plt.savefig(f'{file_prefix}_all_folds_roc_pr_curves.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def model_history(histories, n):\n",
    "    for i, history in enumerate(histories):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "    \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title(f'Model accuracy for fold {i+1}')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title(f'Model loss for fold {i+1}')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "        plt.savefig(f'./output/{n}_fold_{i+1}_accuracy_loss.png')\n",
    "        plt.close()\n",
    "\n",
    "def plot_fold_pr_curves(folds_pr, mean_pr, file_prefix):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for fold_pr in folds_pr:\n",
    "        plt.plot(fold_pr[1], fold_pr[0], lw=1, alpha=0.7)  # recall, precision\n",
    "    \n",
    "    plt.plot(mean_pr[1], mean_pr[0], linestyle='--', lw=2, label='Mean Precision-Recall', color='k')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves for All Folds and Mean PR')\n",
    "    plt.legend(loc='lower left')\n",
    "    \n",
    "    plt.savefig(f'{file_prefix}_folds_pr_curves.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea492e78",
   "metadata": {},
   "source": [
    "## 6. IDC_Conv1D Model Definition\n",
    "\n",
    "This cell defines the convolutional model used in the experiments:\n",
    "- `first_conv_layer`: A feature extractor that applies multi-kernel convolutions, pooling, and dropout to a 650-dimensional input vector.\n",
    "- `create_model`: Builds the full model by applying the convolutional block to both drug and disease feature vectors, concatenating them, and passing the result through fully connected layers to predict a binary label.\n",
    "\n",
    "Each input (drug and disease) is a 650-dimensional vector; together they form the 1300-dimensional `X` input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_conv_layer(input_tensor):\n",
    "    x = Reshape((650, 1))(input_tensor)\n",
    "    x1 = Conv1D(32, 1, activation='relu', padding='same')(x)\n",
    "    x2 = Conv1D(32, 3, activation='relu', padding='same')(x1)\n",
    "    x3 = Conv1D(32, 5, activation='relu', padding='same')(x1)\n",
    "    x4 = MaxPooling1D(pool_size=2)(x)\n",
    "    x5 = Conv1D(32, 1, activation='relu', padding='same')(x4)\n",
    "    x5 = UpSampling1D(size=2)(x5)\n",
    "    x = Concatenate()([x1, x2, x3, x5])\n",
    "    \n",
    "    x = Conv1D(32, 5, activation='relu', padding='same')(x)\n",
    "    x = Conv1D(32, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv1D(32, 1, activation='relu', padding='same')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def create_model():\n",
    "    input1 = Input(shape=(650,))  # Drug features\n",
    "    input2 = Input(shape=(650,))  # Disease features\n",
    "    \n",
    "    x1 = first_conv_layer(input1)\n",
    "    x2 = first_conv_layer(input2)\n",
    "    \n",
    "    concatenated = Concatenate(axis=-1)([x1, x2])\n",
    "    x = Reshape((int(concatenated.shape[1]), 1))(concatenated)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b357d",
   "metadata": {},
   "source": [
    "## 7. Training and Evaluation with Stratified K-Fold Cross-Validation\n",
    "\n",
    "This cell defines the `train_and_evaluate` function, which:\n",
    "- Performs stratified K-fold cross-validation on the training data.\n",
    "- Trains a new `IDC_Conv1D` model on each fold using the weighted loss.\n",
    "- Evaluates each fold on the held-out test set `X_test_0`, `Y_test_0`.\n",
    "- Collects per-fold metrics (AUC-ROC, AUC-PR, accuracy, precision, recall, F1, Brier score, Cohen's kappa, MCC).\n",
    "- Saves ROC/PR plots and per-fold metric summaries to the `./output` directory.\n",
    "\n",
    "Make sure that the `./output` directory exists before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e287c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./output', exist_ok=True)\n",
    "\n",
    "def train_and_evaluate(X, y, X_test, Y_test, learning_rate, number, n_splits=10):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    histories = []\n",
    "    fold_roc_curves = []\n",
    "    fold_pr_curves = []\n",
    "    mean_fpr = np.linspace(0, 1, 650)\n",
    "    mean_tpr = 0.0\n",
    "    mean_recall = np.linspace(0, 1, 650)\n",
    "    mean_precision = 0.0\n",
    "    y_test_preds = []\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X, y), 1):\n",
    "        print(f'Training fold {fold}...')\n",
    "        \n",
    "        x_train, x_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        l_train, r_train = x_train[:, :650], x_train[:, 650:]\n",
    "        l_val, r_val = x_val[:, :650], x_val[:, 650:]\n",
    "        \n",
    "        model = create_model()\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        # Class weights for training data\n",
    "        pos_weight, neg_weight = calculate_class_weights(y_train)\n",
    "        \n",
    "        # Compile with weighted binary cross-entropy loss\n",
    "        model.compile(\n",
    "            loss=lambda y_true, y_pred: weighted_binary_crossentropy_loss(\n",
    "                y_true, y_pred, pos_weight, neg_weight\n",
    "            ),\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=3, verbose=1),\n",
    "            ModelCheckpoint('./output/best_weights.keras', monitor='val_accuracy', save_best_only=True)\n",
    "        ]\n",
    "        \n",
    "        history = model.fit(\n",
    "            [l_train, r_train], y_train,\n",
    "            batch_size=128,\n",
    "            epochs=100,\n",
    "            validation_data=([l_val, r_val], y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        histories.append(history)\n",
    "        \n",
    "        # Test set prediction for current fold\n",
    "        l_test, r_test = X_test[:, :650], X_test[:, 650:]\n",
    "        y_test_pred = model.predict([l_test, r_test])\n",
    "        y_test_preds.append(y_test_pred)\n",
    "\n",
    "        # ROC and PR curves on test set for this fold\n",
    "        fpr, tpr, _ = roc_curve(Y_test, y_test_pred)\n",
    "        precision, recall, _ = precision_recall_curve(Y_test, y_test_pred)\n",
    "        \n",
    "        fold_roc_curves.append((fpr, tpr))\n",
    "        fold_pr_curves.append((precision, recall))\n",
    "\n",
    "        # Accumulate for mean curves\n",
    "        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "        mean_precision += np.interp(mean_recall, recall[::-1], precision[::-1])\n",
    "\n",
    "        # Metrics for this fold\n",
    "        auc_roc = roc_auc_score(Y_test, y_test_pred)\n",
    "        precision_score_value = precision_score(Y_test, (y_test_pred >= 0.5).astype(int))\n",
    "        recall_score_value = recall_score(Y_test, (y_test_pred >= 0.5).astype(int))\n",
    "        accuracy = accuracy_score(Y_test, (y_test_pred >= 0.5).astype(int))\n",
    "        f1 = f1_score(Y_test, (y_test_pred >= 0.5).astype(int))\n",
    "        brier = brier_score_loss(Y_test, y_test_pred)\n",
    "        kappa = cohen_kappa_score(Y_test, (y_test_pred >= 0.5).astype(int))\n",
    "        mcc = matthews_corrcoef(Y_test, (y_test_pred >= 0.5).astype(int))\n",
    "\n",
    "        fold_metric = {\n",
    "            'fold': fold,\n",
    "            'auc-roc': auc_roc,\n",
    "            'auc-pr': auc(recall, precision),\n",
    "            'overall_recall': recall_score_value,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision_score_value,\n",
    "            'Brier score': brier,\n",
    "            'Cohen Kappa': kappa,\n",
    "            'MCC': mcc,\n",
    "            'F1 score': f1\n",
    "        }\n",
    "        fold_metrics.append(fold_metric)\n",
    "\n",
    "    # Save per-fold results\n",
    "    fold_results_df = pd.DataFrame(fold_metrics)\n",
    "    fold_results_df.to_csv('./output/fold_results.csv', index=False)\n",
    "    print('Fold results saved to ./output/fold_results.csv')\n",
    "\n",
    "    # Mean ROC/PR curves\n",
    "    mean_tpr /= n_splits\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_precision /= n_splits\n",
    "    mean_precision[-1] = mean_precision[-2]\n",
    "\n",
    "    model_history(histories, number)\n",
    "\n",
    "    # Plot ROC/PR for all folds (test data predictions)\n",
    "    plot_roc_pr_curves_all_folds(\n",
    "        fold_roc_curves, fold_pr_curves,\n",
    "        mean_fpr, mean_tpr, mean_recall, mean_precision,\n",
    "        f'./output/{number}_test_set'\n",
    "    )\n",
    "\n",
    "    # Evaluate using the last fold's predictions on test set\n",
    "    plot_roc_pr_curves(Y_test, y_test_preds[-1], f'./output/{number}_roc_pr')\n",
    "    \n",
    "    auc_roc = roc_auc_score(Y_test, y_test_preds[-1])\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(Y_test, y_test_preds[-1])\n",
    "    auc_pr = auc(recall_vals, precision_vals)\n",
    "    print(f'auc_roc: {auc_roc}')\n",
    "    print(f'auc_pr: {auc_pr}')\n",
    "\n",
    "    overall_recall = recall_score(Y_test, (y_test_preds[-1] >= 0.5).astype(int))\n",
    "    accuracy = accuracy_score(Y_test, (y_test_preds[-1] >= 0.5).astype(int))\n",
    "    precision_val = precision_score(Y_test, (y_test_preds[-1] >= 0.5).astype(int))\n",
    "\n",
    "    print(f'Overall Recall: {overall_recall}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision_val}')\n",
    "\n",
    "    brier = brier_score_loss(Y_test, y_test_preds[-1])\n",
    "    print(f'Brier score: {brier}')\n",
    "    \n",
    "    y_hat_e = (y_test_preds[-1] >= 0.5).astype(int)\n",
    "    \n",
    "    kappa = cohen_kappa_score(Y_test, y_hat_e)\n",
    "    print(f'Cohen Kappa score: {kappa}')\n",
    "    \n",
    "    mcc = matthews_corrcoef(Y_test, y_hat_e)\n",
    "    print(f'MCC score: {mcc}')\n",
    "    \n",
    "    f1 = f1_score(Y_test, y_hat_e)\n",
    "    print(f'F1 score: {f1}')\n",
    "    \n",
    "    return histories, auc_roc, auc_pr, overall_recall, accuracy, precision_val, brier, kappa, mcc, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c86af4",
   "metadata": {},
   "source": [
    "## 8. Run the Full Experiment\n",
    "\n",
    "This final cell runs the full training and evaluation pipeline:\n",
    "- Iterates over learning rates specified in `param_grid` (here, a single value `0.0001`).\n",
    "- Calls `train_and_evaluate` to perform 10-fold cross-validation and test-set evaluation.\n",
    "- Stores the summary metrics for each configuration into `Results_df.csv` in the `./output` directory.\n",
    "\n",
    "You can adjust the learning rate(s), number of folds, and other hyperparameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "start_time = time.time()\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'number', 'learning_rate', 'auc-roc', 'auc-pr', 'overall_recall',\n",
    "    'accuracy', 'precision', 'Brier score', 'Cohen Kappa', 'MCC', 'F1 score'\n",
    "])\n",
    "\n",
    "param_grid = {'learning_rate': [0.0001]}  # You can add more learning rates here\n",
    "\n",
    "for i, learning_rate in enumerate(param_grid['learning_rate']):\n",
    "    histories, auc_roc, auc_pr, overall_recall, accuracy, precision_val, brier, kappa, mcc, f1 = train_and_evaluate(\n",
    "        X_train_0, y_train_0, X_test_0, Y_test_0, learning_rate, i + 1\n",
    "    )\n",
    "    \n",
    "    new_row = pd.DataFrame({\n",
    "        'number': [i + 1],\n",
    "        'learning_rate': [learning_rate],\n",
    "        'auc-roc': [auc_roc],\n",
    "        'auc-pr': [auc_pr],\n",
    "        'overall_recall': [overall_recall],\n",
    "        'accuracy': [accuracy],\n",
    "        'precision': [precision_val],\n",
    "        'Brier score': [brier],\n",
    "        'Cohen Kappa': [kappa],\n",
    "        'MCC': [mcc],\n",
    "        'F1 score': [f1]\n",
    "    })\n",
    "    print(new_row)\n",
    "\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "results_df.to_csv('./output/Results_df.csv', index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f'Execution time: {execution_time} seconds')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
